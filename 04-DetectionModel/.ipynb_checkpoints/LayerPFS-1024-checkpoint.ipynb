{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc432ee6-0c38-4274-90ed-d86544cef620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from joblib import parallel_backend\n",
    "\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"1\"\n",
    "\n",
    "def _save_json(p, obj):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(p, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def scale_npy_full(input_npy_path, output_npy_path, scaler_out_path, overwrite=False):\n",
    "    input_npy_path = Path(input_npy_path)\n",
    "    output_npy_path = Path(output_npy_path)\n",
    "    scaler_out_path = Path(scaler_out_path)\n",
    "\n",
    "    if not input_npy_path.exists():\n",
    "        raise FileNotFoundError(input_npy_path)\n",
    "    if output_npy_path.exists() and not overwrite:\n",
    "        raise FileExistsError(f\"output exists: {output_npy_path}. Pass overwrite=True to replace.\")\n",
    "\n",
    "    X = np.load(str(input_npy_path))\n",
    "    if X.ndim != 2:\n",
    "        raise RuntimeError(f\"input array must be 2D. got shape={X.shape}\")\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    print(\"Fitting MinMaxScaler on full dataset (in-memory)\")\n",
    "    scaler.fit(X)\n",
    "\n",
    "    print(\"Transforming and saving scaled array\")\n",
    "    X_scaled = scaler.transform(X).astype(np.float32)\n",
    "\n",
    "    output_npy_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.save(str(output_npy_path), X_scaled)\n",
    "\n",
    "    scaler_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(scaler, scaler_out_path)\n",
    "\n",
    "    print(f\"Saved scaled npy -> {output_npy_path}\")\n",
    "    print(f\"Saved scaler -> {scaler_out_path}\")\n",
    "    return str(output_npy_path), str(scaler_out_path)\n",
    "\n",
    "\n",
    "def scale_npy_batched(input_npy_path, output_npy_path, scaler_out_path, batch_size=10000, overwrite=False, use_mmap=True):\n",
    "    input_npy_path = Path(input_npy_path)\n",
    "    output_npy_path = Path(output_npy_path)\n",
    "    scaler_out_path = Path(scaler_out_path)\n",
    "\n",
    "    if not input_npy_path.exists():\n",
    "        raise FileNotFoundError(input_npy_path)\n",
    "    if output_npy_path.exists() and not overwrite:\n",
    "        raise FileExistsError(f\"output exists: {output_npy_path}. Pass overwrite=True to replace.\")\n",
    "\n",
    "    X_in = np.load(str(input_npy_path), mmap_mode='r' if use_mmap else None)\n",
    "    if X_in.ndim != 2:\n",
    "        raise RuntimeError(f\"input array must be 2D. got shape={X_in.shape}\")\n",
    "    n_rows, n_cols = X_in.shape\n",
    "\n",
    "    iterator = range(0, n_rows, batch_size)\n",
    "    if tqdm is not None:\n",
    "        iterator = tqdm(iterator, desc=\"Computing min/max (pass 1)\")\n",
    "\n",
    "    first = True\n",
    "    data_min = None\n",
    "    data_max = None\n",
    "\n",
    "    for start in iterator:\n",
    "        end = min(start + batch_size, n_rows)\n",
    "        batch = np.asarray(X_in[start:end], dtype=np.float64)\n",
    "        if first:\n",
    "            data_min = batch.min(axis=0)\n",
    "            data_max = batch.max(axis=0)\n",
    "            first = False\n",
    "        else:\n",
    "            data_min = np.minimum(data_min, batch.min(axis=0))\n",
    "            data_max = np.maximum(data_max, batch.max(axis=0))\n",
    "\n",
    "    data_range = data_max - data_min\n",
    "    eps = 1e-12\n",
    "    zero_mask = data_range <= 0\n",
    "    if np.any(zero_mask):\n",
    "        data_range[zero_mask] = 1.0\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.data_min_ = data_min\n",
    "    scaler.data_max_ = data_max\n",
    "    scaler.data_range_ = data_range\n",
    "    scaler.scale_ = 1.0 / data_range\n",
    "    scaler.min_ = -data_min * scaler.scale_  \n",
    "\n",
    "    output_npy_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_mm = np.lib.format.open_memmap(str(output_npy_path), mode='w+', dtype=np.float32, shape=(n_rows, n_cols))\n",
    "\n",
    "    iterator2 = range(0, n_rows, batch_size)\n",
    "    if tqdm is not None:\n",
    "        iterator2 = tqdm(iterator2, desc=\"Scaling & writing (pass 2)\")\n",
    "\n",
    "    for start in iterator2:\n",
    "        end = min(start + batch_size, n_rows)\n",
    "        batch = np.asarray(X_in[start:end], dtype=np.float64)\n",
    "        scaled = (batch * scaler.scale_) + scaler.min_\n",
    "        out_mm[start:end] = scaled.astype(np.float32)\n",
    "\n",
    "    scaler_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(scaler, scaler_out_path)\n",
    "\n",
    "    print(f\"Saved scaled memmap npy -> {output_npy_path}\")\n",
    "    print(f\"Saved scaler -> {scaler_out_path}\")\n",
    "    return str(output_npy_path), str(scaler_out_path)\n",
    "\n",
    "\n",
    "\n",
    "def _load_model(model_out_dir):\n",
    "    model_out_dir = Path(model_out_dir)\n",
    "    pipeline_path = model_out_dir / \"pipeline.joblib\"\n",
    "    scaler_path = model_out_dir / \"scaler.joblib\"\n",
    "    clf_path = model_out_dir / \"clf.joblib\"\n",
    "\n",
    "    pipeline = None\n",
    "    scaler = None\n",
    "    clf = None\n",
    "\n",
    "    if pipeline_path.exists():\n",
    "        pipeline = joblib.load(pipeline_path)\n",
    "    else:\n",
    "        if scaler_path.exists():\n",
    "            scaler = joblib.load(scaler_path)\n",
    "        if clf_path.exists():\n",
    "            clf = joblib.load(clf_path)\n",
    "        if scaler is None or clf is None:\n",
    "            raise FileNotFoundError(\"Could not find pipeline or scaler+clf in model_out_dir.\")\n",
    "    return pipeline, scaler, clf\n",
    "\n",
    "\n",
    "def _compute_metrics(y_true, y_pred, y_score):\n",
    "    metrics = {}\n",
    "    metrics[\"accuracy\"] = float(accuracy_score(y_true, y_pred))\n",
    "    metrics[\"precision\"] = float(precision_score(y_true, y_pred, zero_division=0))\n",
    "    metrics[\"recall\"] = float(recall_score(y_true, y_pred, zero_division=0))\n",
    "    metrics[\"f1\"] = float(f1_score(y_true, y_pred, zero_division=0))\n",
    "    try:\n",
    "        if y_score is not None and len(np.unique(y_true)) == 2:\n",
    "            metrics[\"roc_auc\"] = float(roc_auc_score(y_true, y_score))\n",
    "        else:\n",
    "            metrics[\"roc_auc\"] = None\n",
    "    except Exception:\n",
    "        metrics[\"roc_auc\"] = None\n",
    "    metrics[\"confusion_matrix\"] = confusion_matrix(y_true, y_pred).tolist()\n",
    "    metrics[\"classification_report\"] = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _predict_and_score(pipeline, scaler, clf, X_test):\n",
    "    y_score = None\n",
    "    if pipeline is not None:\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        try:\n",
    "            y_score = pipeline.predict_proba(X_test)[:, 1]\n",
    "        except Exception:\n",
    "            try:\n",
    "                clf_ = pipeline.named_steps.get(\"clf\")\n",
    "                scaler_ = pipeline.named_steps.get(\"scaler\")\n",
    "                if clf_ is not None and hasattr(clf_, \"decision_function\"):\n",
    "                    if scaler_ is not None:\n",
    "                        X_s = scaler_.transform(X_test)\n",
    "                    else:\n",
    "                        X_s = X_test\n",
    "                    y_score = clf_.decision_function(X_s)\n",
    "            except Exception:\n",
    "                y_score = None\n",
    "    else:\n",
    "        X_t = scaler.transform(X_test)\n",
    "        y_pred = clf.predict(X_t)\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            y_score = clf.predict_proba(X_t)[:, 1]\n",
    "        elif hasattr(clf, \"decision_function\"):\n",
    "            y_score = clf.decision_function(X_t)\n",
    "    return y_pred, y_score\n",
    "\n",
    "\n",
    "def train_from_npy(\n",
    "    train_npy_path,\n",
    "    model_out_dir,\n",
    "    train_labels_npy_path,\n",
    "    random_seed=42,\n",
    "    mem_limit_bytes=900_000_000_000,\n",
    "    use_mmap=False,\n",
    "    classifier='LogisticRegression',\n",
    "    n_jobs=-1,\n",
    "    scaled=False,\n",
    "):\n",
    "    train_npy_path = Path(train_npy_path)\n",
    "    if not train_npy_path.exists():\n",
    "        raise FileNotFoundError(f\"Train npy not found: {train_npy_path}\")\n",
    "\n",
    "    model_out_dir = Path(model_out_dir)\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    X_train = np.load(str(train_npy_path), mmap_mode=\"r\" if use_mmap else None)\n",
    "    if X_train.ndim != 2:\n",
    "        raise RuntimeError(f\"train array must be 2D. got shape={X_train.shape}\")\n",
    "\n",
    "    labels_path = Path(train_labels_npy_path)\n",
    "    if not labels_path.exists():\n",
    "        raise FileNotFoundError(f\"Provided train_labels_npy_path does not exist: {labels_path}\")\n",
    "    y_train = np.load(str(labels_path), mmap_mode=\"r\" if use_mmap else None)\n",
    "    if y_train.shape[0] != X_train.shape[0]:\n",
    "        raise RuntimeError(f\"train labels length ({y_train.shape[0]}) != train rows ({X_train.shape[0]})\")\n",
    "\n",
    "    y_train = y_train.astype(np.int64)\n",
    "    unique = np.unique(y_train)\n",
    "    if unique.size != 2 or not np.array_equal(np.sort(unique), np.array([0, 1])):\n",
    "        raise RuntimeError(f\"train labels must contain exactly two classes 0 and 1. found: {unique}\")\n",
    "    y_train = y_train.astype(np.uint8)\n",
    "\n",
    "    estimated_bytes = getattr(X_train, \"nbytes\", X_train.size * X_train.itemsize)\n",
    "    if estimated_bytes > mem_limit_bytes:\n",
    "        raise MemoryError(\n",
    "            f\"Estimated train bytes {estimated_bytes} > mem_limit_bytes {mem_limit_bytes}. \"\n",
    "            \"Either increase mem_limit_bytes, reduce dataset size, or use use_mmap=True and implement chunked/out-of-core training.\"\n",
    "        )\n",
    "\n",
    "    if classifier == 'LogisticRegression':\n",
    "        print('Using LogisticRegression as classifier')\n",
    "        clf = LogisticRegression(random_state=random_seed, n_jobs=n_jobs, verbose=1)\n",
    "    elif classifier == 'RandomForest':\n",
    "        print('Using RandomForest as classifier')\n",
    "        clf = RandomForestClassifier(random_state=random_seed, n_jobs=n_jobs, verbose=1)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Pick either Logistic Regression or Random Forest as the classifier\") \n",
    "\n",
    "    if scaled:\n",
    "        pipeline = Pipeline([(\"clf\", clf)])\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "        pipeline = Pipeline([(\"scaler\", scaler), (\"clf\", clf)])\n",
    "\n",
    "    print(f\"Fitting pipeline\")\n",
    "    with parallel_backend(\"threading\"):\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "    try:\n",
    "        y_pred_train, y_score_train = _predict_and_score(pipeline, None, None, X_train)\n",
    "        train_metrics = _compute_metrics(y_train, y_pred_train, y_score_train)\n",
    "\n",
    "        print(\"TRAIN Eval results:\")\n",
    "        print(f\"  acc={train_metrics['accuracy']:.4f}  prec={train_metrics['precision']:.4f}  \"\n",
    "              f\"recall={train_metrics['recall']:.4f}  f1={train_metrics['f1']:.4f}\")\n",
    "        if train_metrics.get(\"roc_auc\") is not None:\n",
    "            print(f\"  roc_auc={train_metrics['roc_auc']:.4f}\")\n",
    "        print(\"  confusion_matrix:\", train_metrics[\"confusion_matrix\"])\n",
    "    except Exception as e:\n",
    "        print(\"Warning: failed to compute train metrics:\", e)\n",
    "        train_metrics = None\n",
    "\n",
    "    pipeline_path = model_out_dir / \"pipeline.joblib\"\n",
    "    scaler_path = model_out_dir / \"scaler.joblib\"\n",
    "    clf_path = model_out_dir / \"clf.joblib\"\n",
    "    meta_out = model_out_dir / \"train_meta.json\"\n",
    "\n",
    "    joblib.dump(pipeline, pipeline_path)\n",
    "    try:\n",
    "        if hasattr(pipeline, 'named_steps') and 'scaler' in pipeline.named_steps:\n",
    "            joblib.dump(pipeline.named_steps['scaler'], scaler_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    joblib.dump(pipeline.named_steps['clf'], clf_path)\n",
    "\n",
    "    meta_obj = {\n",
    "        \"train_npy\": str(train_npy_path),\n",
    "        \"train_labels_npy\": str(train_labels_npy_path),\n",
    "        \"train_rows\": int(X_train.shape[0]),\n",
    "        \"scaled\": bool(scaled),\n",
    "        \"random_seed\": int(random_seed),\n",
    "        \"use_mmap\": bool(use_mmap),\n",
    "        \"train_metrics\": train_metrics,\n",
    "    }\n",
    "    _save_json(meta_out, meta_obj)\n",
    "\n",
    "    print(\"Saved pipeline ->\", pipeline_path)\n",
    "    if (model_out_dir / \"scaler.joblib\").exists():\n",
    "        print(\"Saved scaler ->\", scaler_path)\n",
    "    print(\"Saved classifier ->\", clf_path)\n",
    "    return str(model_out_dir)\n",
    "\n",
    "def eval_val_from_npy(\n",
    "    val_npy_path,\n",
    "    val_label_path,\n",
    "    model_out_dir=\"./models\",\n",
    "    return_predictions=False,\n",
    "    use_mmap=False,\n",
    "):\n",
    "    val_npy_path = Path(val_npy_path)\n",
    "    val_label_path = Path(val_label_path)\n",
    "    if not val_npy_path.exists():\n",
    "        raise FileNotFoundError(f\"val_npy not found: {val_npy_path}\")\n",
    "\n",
    "    X_test = np.load(str(val_npy_path), mmap_mode=\"r\" if use_mmap else None).astype(np.float32)\n",
    "    y_test = np.load(str(val_label_path), mmap_mode=\"r\" if use_mmap else None).astype(np.uint8)\n",
    "\n",
    "    if X_test.shape[0] != y_test.shape[0]:\n",
    "        raise RuntimeError(f\"val X rows ({X_test.shape[0]}) != val labels length ({y_test.shape[0]})\")\n",
    "\n",
    "    pipeline, scaler, clf = _load_model(model_out_dir)\n",
    "\n",
    "    y_pred, y_score = _predict_and_score(pipeline, scaler, clf, X_test)\n",
    "\n",
    "    metrics = _compute_metrics(y_test, y_pred, y_score)\n",
    "\n",
    "    print(\"VAL Eval results:\")\n",
    "    print(f\"  acc={metrics['accuracy']:.4f}  prec={metrics['precision']:.4f}  recall={metrics['recall']:.4f}  f1={metrics['f1']:.4f}\")\n",
    "    if metrics[\"roc_auc\"] is not None:\n",
    "        print(f\"  roc_auc={metrics['roc_auc']:.4f}\")\n",
    "    print(\"  confusion_matrix:\", metrics[\"confusion_matrix\"])\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"y_true\": y_test if return_predictions else None,\n",
    "        \"y_pred\": y_pred if return_predictions else None,\n",
    "        \"y_score\": y_score if return_predictions else None,\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_test_from_npy(\n",
    "    test_clean_npy_path,\n",
    "    test_adv_npy_path=None,\n",
    "    model_out_dir=\"./models\",\n",
    "    test_labels_npy_path=None,\n",
    "    return_predictions=False,\n",
    "    use_mmap=False,\n",
    "):\n",
    "    test_clean_npy_path = Path(test_clean_npy_path)\n",
    "    if not test_clean_npy_path.exists():\n",
    "        raise FileNotFoundError(f\"test_clean npy not found: {test_clean_npy_path}\")\n",
    "\n",
    "    if test_adv_npy_path is None:\n",
    "        if test_labels_npy_path is None:\n",
    "            raise ValueError(\"Single-file mode requires test_labels_npy_path.\")\n",
    "        X_test = np.load(str(test_clean_npy_path), mmap_mode=\"r\" if use_mmap else None).astype(np.float32)\n",
    "        y_test = np.load(str(test_labels_npy_path), mmap_mode=\"r\" if use_mmap else None).astype(np.uint8)\n",
    "        if X_test.shape[0] != y_test.shape[0]:\n",
    "            raise RuntimeError(f\"test X rows ({X_test.shape[0]}) != test labels length ({y_test.shape[0]})\")\n",
    "    else:\n",
    "        test_adv_npy_path = Path(test_adv_npy_path)\n",
    "        if not test_adv_npy_path.exists():\n",
    "            raise FileNotFoundError(f\"test_adv npy not found: {test_adv_npy_path}\")\n",
    "        X_clean = np.load(str(test_clean_npy_path), mmap_mode=\"r\" if use_mmap else None).astype(np.float32)\n",
    "        X_adv = np.load(str(test_adv_npy_path), mmap_mode=\"r\" if use_mmap else None).astype(np.float32)\n",
    "        X_test = np.vstack([X_clean, X_adv])\n",
    "        if test_labels_npy_path is not None:\n",
    "            y_test = np.load(str(test_labels_npy_path), mmap_mode=\"r\" if use_mmap else None).astype(np.uint8)\n",
    "            if y_test.shape[0] != X_test.shape[0]:\n",
    "                raise RuntimeError(f\"test labels length ({y_test.shape[0]}) != stacked test rows ({X_test.shape[0]})\")\n",
    "        else:\n",
    "            y_test = np.concatenate([np.zeros(X_clean.shape[0], dtype=np.uint8),\n",
    "                                     np.ones(X_adv.shape[0], dtype=np.uint8)])\n",
    "\n",
    "    pipeline, scaler, clf = _load_model(model_out_dir)\n",
    "\n",
    "    y_pred, y_score = _predict_and_score(pipeline, scaler, clf, X_test)\n",
    "\n",
    "    metrics = _compute_metrics(y_test, y_pred, y_score)\n",
    "\n",
    "    print(\"TEST Eval results:\")\n",
    "    print(f\"  acc={metrics['accuracy']:.4f}  prec={metrics['precision']:.4f}  recall={metrics['recall']:.4f}  f1={metrics['f1']:.4f}\")\n",
    "    if metrics[\"roc_auc\"] is not None:\n",
    "        print(f\"  roc_auc={metrics['roc_auc']:.4f}\")\n",
    "    print(\"  confusion_matrix:\", metrics[\"confusion_matrix\"])\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"y_true\": y_test if return_predictions else None,\n",
    "        \"y_pred\": y_pred if return_predictions else None,\n",
    "        \"y_score\": y_score if return_predictions else None,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3977a5-1070-4a84-ac8a-58a4129d81d3",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677317fd-102f-438f-ad2a-354a2849ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "\n",
    "def _save_json(p, obj):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(p, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "\n",
    "def _load_model(model_out_dir, scaler_path = None):\n",
    "    model_out_dir = Path(model_out_dir)\n",
    "    pipeline_path = model_out_dir / \"pipeline.joblib\"\n",
    "    clf_path = model_out_dir / \"clf.joblib\"\n",
    "\n",
    "    if scaler_path == None:\n",
    "        scaler_path = model_out_dir / \"scaler.joblib\"\n",
    "    else:\n",
    "        scaler_path = Path(scaler_path)\n",
    "\n",
    "    pipeline = None\n",
    "    scaler = None\n",
    "    clf = None\n",
    "\n",
    "    if pipeline_path.exists():\n",
    "        pipeline = joblib.load(pipeline_path)\n",
    "    else:\n",
    "        if scaler_path.exists():\n",
    "            scaler = joblib.load(scaler_path)\n",
    "        if clf_path.exists():\n",
    "            clf = joblib.load(clf_path)\n",
    "        if scaler is None or clf is None:\n",
    "            raise FileNotFoundError(\"Could not find pipeline or scaler+clf in model_out_dir.\")\n",
    "    return pipeline, scaler, clf\n",
    "\n",
    "\n",
    "def _compute_metrics(y_true, y_pred, y_score):\n",
    "    metrics = {}\n",
    "    metrics[\"accuracy\"] = float(accuracy_score(y_true, y_pred))\n",
    "    metrics[\"precision\"] = float(precision_score(y_true, y_pred, zero_division=0))\n",
    "    metrics[\"recall\"] = float(recall_score(y_true, y_pred, zero_division=0))\n",
    "    metrics[\"f1\"] = float(f1_score(y_true, y_pred, zero_division=0))\n",
    "    try:\n",
    "        if y_score is not None and len(np.unique(y_true)) == 2:\n",
    "            metrics[\"roc_auc\"] = float(roc_auc_score(y_true, y_score))\n",
    "        else:\n",
    "            metrics[\"roc_auc\"] = None\n",
    "    except Exception:\n",
    "        metrics[\"roc_auc\"] = None\n",
    "    metrics[\"confusion_matrix\"] = confusion_matrix(y_true, y_pred).tolist()\n",
    "    metrics[\"classification_report\"] = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _predict_and_score(pipeline, scaler, clf, X_test):\n",
    "    y_score = None\n",
    "    if pipeline is not None:\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        if hasattr(pipeline, \"predict_proba\"):\n",
    "            try:\n",
    "                y_score = pipeline.predict_proba(X_test)[:, 1]\n",
    "            except Exception:\n",
    "                y_score = None\n",
    "        else:\n",
    "            try:\n",
    "                clf_ = pipeline.named_steps[\"clf\"]\n",
    "                scaler_ = pipeline.named_steps[\"scaler\"]\n",
    "                if hasattr(clf_, \"decision_function\"):\n",
    "                    y_score = clf_.decision_function(scaler_.transform(X_test))\n",
    "            except Exception:\n",
    "                y_score = None\n",
    "    else:\n",
    "        X_t = scaler.transform(X_test)\n",
    "        y_pred = clf.predict(X_t)\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            y_score = clf.predict_proba(X_t)[:, 1]\n",
    "        elif hasattr(clf, \"decision_function\"):\n",
    "            y_score = clf.decision_function(X_t)\n",
    "    return y_pred, y_score\n",
    "\n",
    "\n",
    "def eval_val_from_npy(\n",
    "    val_npy_path,\n",
    "    val_label_path,\n",
    "    model_out_dir=\"./models\",\n",
    "    return_predictions=False,\n",
    "    use_mmap=False,\n",
    "    csv_path=None,\n",
    "    scaler_path=None\n",
    "):\n",
    "    val_npy_path = Path(val_npy_path)\n",
    "    val_label_path = Path(val_label_path)\n",
    "    if not val_npy_path.exists():\n",
    "        raise FileNotFoundError(f\"val_npy not found: {val_npy_path}\")\n",
    "\n",
    "    X_test = np.load(str(val_npy_path), mmap_mode=\"r\" if use_mmap else None).astype(np.float32)\n",
    "    y_test = np.load(str(val_label_path), mmap_mode=\"r\" if use_mmap else None).astype(np.uint8)\n",
    "    \n",
    "    if X_test.shape[0] != y_test.shape[0]:\n",
    "        raise RuntimeError(f\"val X rows ({X_test.shape[0]}) != val labels length ({y_test.shape[0]})\")\n",
    "\n",
    "    pipeline, scaler, clf = _load_model(model_out_dir, scaler_path=scaler_path)\n",
    "\n",
    "    y_pred, y_score = _predict_and_score(pipeline, scaler, clf, X_test)\n",
    "\n",
    "    metrics = _compute_metrics(y_test, y_pred, y_score)\n",
    "\n",
    "    print(\"VAL Eval results:\")\n",
    "    print(f\"  acc={metrics['accuracy']:.4f}  prec={metrics['precision']:.4f}  recall={metrics['recall']:.4f}  f1={metrics['f1']:.4f}\")\n",
    "    if metrics[\"roc_auc\"] is not None:\n",
    "        print(f\"  roc_auc={metrics['roc_auc']:.4f}\")\n",
    "    print(\"  confusion_matrix:\", metrics[\"confusion_matrix\"])\n",
    "\n",
    "    if csv_path is not None:\n",
    "        csv_path = Path(csv_path)\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        metrics_df = pd.DataFrame([metrics])\n",
    "        metrics_df.to_csv(csv_path, index=False)\n",
    "\n",
    "        if return_predictions:\n",
    "            preds_path = csv_path.with_name(csv_path.stem + \"_predictions.csv\")\n",
    "            preds_df = pd.DataFrame({\n",
    "                \"y_true\": y_test,\n",
    "                \"y_pred\": y_pred,\n",
    "                \"y_score\": y_score\n",
    "            })\n",
    "            preds_df.to_csv(preds_path, index=False)\n",
    "            print(f\"Predictions saved to: {preds_path}\")\n",
    "\n",
    "        print(f\"Metrics saved to: {csv_path}\")\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"y_true\": y_test if return_predictions else None,\n",
    "        \"y_pred\": y_pred if return_predictions else None,\n",
    "        \"y_score\": y_score if return_predictions else None,\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_test_from_npy(\n",
    "    test_clean_npy_path,\n",
    "    test_adv_npy_path=None,\n",
    "    model_out_dir=\"./models\",\n",
    "    test_labels_npy_path=None,\n",
    "    return_predictions=False,\n",
    "    use_mmap=False,\n",
    "    csv_path=None,  \n",
    "    scaler_path=None\n",
    "):\n",
    "    test_clean_npy_path = Path(test_clean_npy_path)\n",
    "    if not test_clean_npy_path.exists():\n",
    "        raise FileNotFoundError(f\"test_clean npy not found: {test_clean_npy_path}\")\n",
    "\n",
    "    if test_adv_npy_path is None:\n",
    "        if test_labels_npy_path is None:\n",
    "            raise ValueError(\"Single-file mode requires test_labels_npy_path.\")\n",
    "        X_test = np.load(str(test_clean_npy_path), mmap_mode=\"r\" if use_mmap else None).astype(np.float32)\n",
    "        y_test = np.load(str(test_labels_npy_path), mmap_mode=\"r\" if use_mmap else None).astype(np.uint8)\n",
    "        if X_test.shape[0] != y_test.shape[0]:\n",
    "            raise RuntimeError(f\"test X rows ({X_test.shape[0]}) != test labels length ({y_test.shape[0]})\")\n",
    "    else:\n",
    "        test_adv_npy_path = Path(test_adv_npy_path)\n",
    "        if not test_adv_npy_path.exists():\n",
    "            raise FileNotFoundError(f\"test_adv npy not found: {test_adv_npy_path}\")\n",
    "        X_clean = np.load(str(test_clean_npy_path), mmap_mode=\"r\" if use_mmap else None).astype(np.float32)\n",
    "        X_adv = np.load(str(test_adv_npy_path), mmap_mode=\"r\" if use_mmap else None).astype(np.float32)\n",
    "        X_test = np.vstack([X_clean, X_adv])\n",
    "        if test_labels_npy_path is not None:\n",
    "            y_test = np.load(str(test_labels_npy_path), mmap_mode=\"r\" if use_mmap else None).astype(np.uint8)\n",
    "            if y_test.shape[0] != X_test.shape[0]:\n",
    "                raise RuntimeError(f\"test labels length ({y_test.shape[0]}) != stacked test rows ({X_test.shape[0]})\")\n",
    "        else:\n",
    "            y_test = np.concatenate([\n",
    "                np.zeros(X_clean.shape[0], dtype=np.uint8),\n",
    "                np.ones(X_adv.shape[0], dtype=np.uint8)\n",
    "            ])\n",
    "\n",
    "    pipeline, scaler, clf = _load_model(model_out_dir, scaler_path=scaler_path)\n",
    "    y_pred, y_score = _predict_and_score(pipeline, scaler, clf, X_test)\n",
    "    metrics = _compute_metrics(y_test, y_pred, y_score)\n",
    "\n",
    "    print(\"TEST Eval results:\")\n",
    "    print(f\"  acc={metrics['accuracy']:.4f}  prec={metrics['precision']:.4f}  recall={metrics['recall']:.4f}  f1={metrics['f1']:.4f}\")\n",
    "    if metrics[\"roc_auc\"] is not None:\n",
    "        print(f\"  roc_auc={metrics['roc_auc']:.4f}\")\n",
    "\n",
    "    if csv_path is not None:\n",
    "        csv_path = Path(csv_path)\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        metrics_df = pd.DataFrame([metrics])\n",
    "        metrics_df.to_csv(csv_path, index=False)\n",
    "\n",
    "        if return_predictions:\n",
    "            preds_path = csv_path.with_name(csv_path.stem + \"_predictions.csv\")\n",
    "            preds_df = pd.DataFrame({\n",
    "                \"y_true\": y_test,\n",
    "                \"y_pred\": y_pred,\n",
    "                \"y_score\": y_score\n",
    "            })\n",
    "            preds_df.to_csv(preds_path, index=False)\n",
    "            print(f\"Predictions saved to: {preds_path}\")\n",
    "\n",
    "        print(f\"Metrics saved to: {csv_path}\")\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6275df45-ec92-4390-9deb-cb8d0992598a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cadbe49-ac14-41c8-8ec5-2887287e53cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LogisticRegression as classifier\n",
      "Fitting pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "  confusion_matrix: [[486, 0], [0, 486]]\n",
      "Saved pipeline -> fgsm/model-LogisticRegression/pipeline.joblib\n",
      "Saved classifier -> fgsm/model-LogisticRegression/clf.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fgsm/model-LogisticRegression'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTACK = 'fgsm'\n",
    "IMG_SIZE = 1024\n",
    "classifier='LogisticRegression'\n",
    "\n",
    "train_from_npy(\n",
    "    train_npy_path=f\"./{ATTACK}/train_raw_scaled.npy\",\n",
    "    model_out_dir=f\"./{ATTACK}/model-{classifier}\",\n",
    "    train_labels_npy_path=f\"./{ATTACK}/train_labels.npy\",\n",
    "    random_seed=42,\n",
    "    use_mmap=False,\n",
    "    classifier=classifier,\n",
    "    n_jobs=1,\n",
    "    scaled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69eb2033-ca34-4ea4-8b71-4b0f5b98a9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RandomForest as classifier\n",
      "Fitting pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   34.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "  confusion_matrix: [[486, 0], [0, 486]]\n",
      "Saved pipeline -> fgsm/model-RandomForest/pipeline.joblib\n",
      "Saved classifier -> fgsm/model-RandomForest/clf.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fgsm/model-RandomForest'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTACK = 'fgsm'\n",
    "IMG_SIZE = 1024\n",
    "classifier='RandomForest'\n",
    "\n",
    "train_from_npy(\n",
    "    train_npy_path=f\"./{ATTACK}/train_raw_scaled.npy\",\n",
    "    model_out_dir=f\"./{ATTACK}/model-{classifier}\",\n",
    "    train_labels_npy_path=f\"./{ATTACK}/train_labels.npy\",\n",
    "    random_seed=42,\n",
    "    use_mmap=False,\n",
    "    classifier=classifier,\n",
    "    n_jobs=4,\n",
    "    scaled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd7f4b-fe95-42cb-b177-32f0fe94752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK = 'bim'\n",
    "IMG_SIZE = 1024\n",
    "classifier='LogisticRegression'\n",
    "\n",
    "train_from_npy(\n",
    "    train_npy_path=f\"./{ATTACK}/train_raw_scaled.npy\",\n",
    "    model_out_dir=f\"./{ATTACK}/model-{classifier}\",\n",
    "    train_labels_npy_path=f\"./{ATTACK}/train_labels.npy\",\n",
    "    random_seed=42,\n",
    "    use_mmap=False,\n",
    "    classifier=classifier,\n",
    "    n_jobs=1,\n",
    "    scaled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b022ff6c-5498-406c-92c3-8ccef70369d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RandomForest as classifier\n",
      "Fitting pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   28.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "  confusion_matrix: [[486, 0], [0, 486]]\n",
      "Saved pipeline -> bim/model-RandomForest/pipeline.joblib\n",
      "Saved classifier -> bim/model-RandomForest/clf.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bim/model-RandomForest'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTACK = 'bim'\n",
    "IMG_SIZE = 1024\n",
    "classifier='RandomForest'\n",
    "\n",
    "train_from_npy(\n",
    "    train_npy_path=f\"./{ATTACK}/train_raw_scaled.npy\",\n",
    "    model_out_dir=f\"./{ATTACK}/model-{classifier}\",\n",
    "    train_labels_npy_path=f\"./{ATTACK}/train_labels.npy\",\n",
    "    random_seed=42,\n",
    "    use_mmap=False,\n",
    "    classifier=classifier,\n",
    "    n_jobs=4,\n",
    "    scaled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d64ac7d-698b-4712-a92f-9af3bac96a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LogisticRegression as classifier\n",
      "Fitting pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "  confusion_matrix: [[486, 0], [0, 486]]\n",
      "Saved pipeline -> pgd/model-LogisticRegression/pipeline.joblib\n",
      "Saved classifier -> pgd/model-LogisticRegression/clf.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pgd/model-LogisticRegression'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTACK = 'pgd'\n",
    "IMG_SIZE = 1024\n",
    "classifier='LogisticRegression'\n",
    "\n",
    "train_from_npy(\n",
    "    train_npy_path=f\"./{ATTACK}/train_raw_scaled.npy\",\n",
    "    model_out_dir=f\"./{ATTACK}/model-{classifier}\",\n",
    "    train_labels_npy_path=f\"./{ATTACK}/train_labels.npy\",\n",
    "    random_seed=42,\n",
    "    use_mmap=False,\n",
    "    classifier=classifier,\n",
    "    n_jobs=1,\n",
    "    scaled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3b26ed-bda1-4467-bf12-f02505076d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RandomForest as classifier\n",
      "Fitting pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   27.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "  confusion_matrix: [[486, 0], [0, 486]]\n",
      "Saved pipeline -> pgd/model-RandomForest/pipeline.joblib\n",
      "Saved classifier -> pgd/model-RandomForest/clf.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pgd/model-RandomForest'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTACK = 'pgd'\n",
    "IMG_SIZE = 1024\n",
    "classifier='RandomForest'\n",
    "\n",
    "train_from_npy(\n",
    "    train_npy_path=f\"./{ATTACK}/train_raw_scaled.npy\",\n",
    "    model_out_dir=f\"./{ATTACK}/model-{classifier}\",\n",
    "    train_labels_npy_path=f\"./{ATTACK}/train_labels.npy\",\n",
    "    random_seed=42,\n",
    "    use_mmap=False,\n",
    "    classifier=classifier,\n",
    "    n_jobs=4,\n",
    "    scaled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7ce246-ba4b-472c-9c8e-6facdc183c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LogisticRegression as classifier\n",
      "Fitting pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/kaggle_ml/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 37.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Eval results:\n",
      "  acc=0.9455  prec=0.9391  recall=0.9527  f1=0.9459\n",
      "  roc_auc=0.9837\n",
      "  confusion_matrix: [[456, 30], [23, 463]]\n",
      "Saved pipeline -> df/model-LogisticRegression/pipeline.joblib\n",
      "Saved classifier -> df/model-LogisticRegression/clf.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'df/model-LogisticRegression'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTACK = 'df'\n",
    "IMG_SIZE = 1024\n",
    "classifier='LogisticRegression'\n",
    "\n",
    "train_from_npy(\n",
    "    train_npy_path=f\"./{ATTACK}/train_raw_scaled.npy\",\n",
    "    model_out_dir=f\"./{ATTACK}/model-{classifier}\",\n",
    "    train_labels_npy_path=f\"./{ATTACK}/train_labels.npy\",\n",
    "    random_seed=42,\n",
    "    use_mmap=False,\n",
    "    classifier=classifier,\n",
    "    n_jobs=1,\n",
    "    scaled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f17f40-240f-4925-82e7-76fa06c37d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RandomForest as classifier\n",
      "Fitting pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  2.8min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "  confusion_matrix: [[486, 0], [0, 486]]\n",
      "Saved pipeline -> df/model-RandomForest/pipeline.joblib\n",
      "Saved classifier -> df/model-RandomForest/clf.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'df/model-RandomForest'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTACK = 'df'\n",
    "IMG_SIZE = 1024\n",
    "classifier='RandomForest'\n",
    "\n",
    "train_from_npy(\n",
    "    train_npy_path=f\"./{ATTACK}/train_raw_scaled.npy\",\n",
    "    model_out_dir=f\"./{ATTACK}/model-{classifier}\",\n",
    "    train_labels_npy_path=f\"./{ATTACK}/train_labels.npy\",\n",
    "    random_seed=42,\n",
    "    use_mmap=False,\n",
    "    classifier=classifier,\n",
    "    n_jobs=4,\n",
    "    scaled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab27a0d-166e-471e-9ce1-5f890ed0fcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LogisticRegression as classifier\n",
      "Fitting pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/kaggle_ml/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 33.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "  confusion_matrix: [[486, 0], [0, 486]]\n",
      "Saved pipeline -> cw/model-LogisticRegression/pipeline.joblib\n",
      "Saved classifier -> cw/model-LogisticRegression/clf.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cw/model-LogisticRegression'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTACK = 'cw'\n",
    "IMG_SIZE = 1024\n",
    "classifier='LogisticRegression'\n",
    "\n",
    "train_from_npy(\n",
    "    train_npy_path=f\"/mnt/ephemeral0/{ATTACK}/train_raw_scaled.npy\",\n",
    "    model_out_dir=f\"./{ATTACK}/model-{classifier}\",\n",
    "    train_labels_npy_path=f\"/mnt/ephemeral0/{ATTACK}/train_labels.npy\",\n",
    "    random_seed=42,\n",
    "    use_mmap=False,\n",
    "    classifier=classifier,\n",
    "    n_jobs=1,\n",
    "    scaled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eac8e5a1-37ac-47d6-bd46-3807d85046ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RandomForest as classifier\n",
      "Fitting pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  2.8min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "  confusion_matrix: [[486, 0], [0, 486]]\n",
      "Saved pipeline -> cw/model-RandomForest/pipeline.joblib\n",
      "Saved classifier -> cw/model-RandomForest/clf.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cw/model-RandomForest'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTACK = 'cw'\n",
    "IMG_SIZE = 1024\n",
    "classifier='RandomForest'\n",
    "\n",
    "train_from_npy(\n",
    "    train_npy_path=f\"/mnt/ephemeral0/{ATTACK}/train_raw_scaled.npy\",\n",
    "    model_out_dir=f\"./{ATTACK}/model-{classifier}\",\n",
    "    train_labels_npy_path=f\"/mnt/ephemeral0/{ATTACK}/train_labels.npy\",\n",
    "    random_seed=42,\n",
    "    use_mmap=False,\n",
    "    classifier=classifier,\n",
    "    n_jobs=4,\n",
    "    scaled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462ecf0-e8dc-4d08-8b0f-eac80e3d9998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb186fe-8b9b-4f80-9d52-ad7d6da13454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58a48134-de96-4413-9a9c-b673df89ea33",
   "metadata": {},
   "source": [
    "# Evaluate Against Val Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d12b7a5e-a16e-40be-a6cc-363750139fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Method: fgsm\n",
      "\n",
      "\n",
      " Classifier: RandomForest: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL Eval results:\n",
      "  acc=0.4259  prec=0.4565  recall=0.7778  f1=0.5753\n",
      "  roc_auc=0.0870\n",
      "  confusion_matrix: [[6, 75], [18, 63]]\n",
      "Predictions saved to: eval-VALSETx/fgsm-fgsm-1024-RandomForest_test_result_predictions.csv\n",
      "Metrics saved to: eval-VALSETx/fgsm-fgsm-1024-RandomForest_test_result.csv\n",
      "val metrics: {'accuracy': 0.42592592592592593, 'precision': 0.45652173913043476, 'recall': 0.7777777777777778, 'f1': 0.5753424657534246, 'roc_auc': 0.08702941624752324, 'confusion_matrix': [[6, 75], [18, 63]], 'classification_report': {'0': {'precision': 0.25, 'recall': 0.07407407407407407, 'f1-score': 0.11428571428571428, 'support': 81.0}, '1': {'precision': 0.45652173913043476, 'recall': 0.7777777777777778, 'f1-score': 0.5753424657534246, 'support': 81.0}, 'accuracy': 0.42592592592592593, 'macro avg': {'precision': 0.3532608695652174, 'recall': 0.42592592592592593, 'f1-score': 0.3448140900195695, 'support': 162.0}, 'weighted avg': {'precision': 0.3532608695652174, 'recall': 0.42592592592592593, 'f1-score': 0.3448140900195694, 'support': 162.0}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "ATTACK = 'fgsm'\n",
    "classifiers = ['RandomForest']\n",
    "IMG_SIZE = 1024\n",
    "\n",
    "print (f'Attack Method: {ATTACK}\\n')\n",
    "for classifier in classifiers:\n",
    "    print (f'\\n Classifier: {classifier}: \\n')\n",
    "    val_res = eval_val_from_npy(\n",
    "         val_npy_path = f\"/mnt/ephemeral0/{ATTACK}/val_raw.npy\",\n",
    "         val_label_path = f\"/mnt/ephemeral0/{ATTACK}/val_labels.npy\",\n",
    "         model_out_dir = f\"./{ATTACK}/model-{classifier}-initialscript\",\n",
    "         return_predictions=True,\n",
    "         use_mmap=False,\n",
    "         csv_path=f\"./eval-VALSETx/{ATTACK}-{ATTACK}-{IMG_SIZE}-{classifier}_test_result.csv\",\n",
    "         scaler_path=f\"./{ATTACK}/scaler.joblib\"\n",
    "    )\n",
    "    print(\"val metrics:\", val_res[\"metrics\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34c194a3-f013-4225-9510-a54795abf9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Method: fgsm\n",
      "\n",
      "\n",
      " Classifier: LogisticRegression: \n",
      "\n",
      "VAL Eval results:\n",
      "  acc=0.9568  prec=0.9405  recall=0.9753  f1=0.9576\n",
      "  roc_auc=0.9979\n",
      "  confusion_matrix: [[76, 5], [2, 79]]\n",
      "Predictions saved to: eval-VALSET/fgsm-fgsm-1024-LogisticRegression_test_result_predictions.csv\n",
      "Metrics saved to: eval-VALSET/fgsm-fgsm-1024-LogisticRegression_test_result.csv\n",
      "val metrics: {'accuracy': 0.9567901234567902, 'precision': 0.9404761904761905, 'recall': 0.9753086419753086, 'f1': 0.9575757575757575, 'roc_auc': 0.9978661789361378, 'confusion_matrix': [[76, 5], [2, 79]], 'classification_report': {'0': {'precision': 0.9743589743589743, 'recall': 0.9382716049382716, 'f1-score': 0.9559748427672956, 'support': 81.0}, '1': {'precision': 0.9404761904761905, 'recall': 0.9753086419753086, 'f1-score': 0.9575757575757575, 'support': 81.0}, 'accuracy': 0.9567901234567902, 'macro avg': {'precision': 0.9574175824175823, 'recall': 0.9567901234567902, 'f1-score': 0.9567753001715266, 'support': 162.0}, 'weighted avg': {'precision': 0.9574175824175825, 'recall': 0.9567901234567902, 'f1-score': 0.9567753001715266, 'support': 162.0}}}\n",
      "\n",
      " Classifier: RandomForest: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL Eval results:\n",
      "  acc=0.4259  prec=0.4565  recall=0.7778  f1=0.5753\n",
      "  roc_auc=0.0870\n",
      "  confusion_matrix: [[6, 75], [18, 63]]\n",
      "Predictions saved to: eval-VALSET/fgsm-fgsm-1024-RandomForest_test_result_predictions.csv\n",
      "Metrics saved to: eval-VALSET/fgsm-fgsm-1024-RandomForest_test_result.csv\n",
      "val metrics: {'accuracy': 0.42592592592592593, 'precision': 0.45652173913043476, 'recall': 0.7777777777777778, 'f1': 0.5753424657534246, 'roc_auc': 0.08702941624752324, 'confusion_matrix': [[6, 75], [18, 63]], 'classification_report': {'0': {'precision': 0.25, 'recall': 0.07407407407407407, 'f1-score': 0.11428571428571428, 'support': 81.0}, '1': {'precision': 0.45652173913043476, 'recall': 0.7777777777777778, 'f1-score': 0.5753424657534246, 'support': 81.0}, 'accuracy': 0.42592592592592593, 'macro avg': {'precision': 0.3532608695652174, 'recall': 0.42592592592592593, 'f1-score': 0.3448140900195695, 'support': 162.0}, 'weighted avg': {'precision': 0.3532608695652174, 'recall': 0.42592592592592593, 'f1-score': 0.3448140900195694, 'support': 162.0}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "ATTACK = 'fgsm'\n",
    "classifiers = ['LogisticRegression', 'RandomForest']\n",
    "IMG_SIZE = 1024\n",
    "\n",
    "print (f'Attack Method: {ATTACK}\\n')\n",
    "for classifier in classifiers:\n",
    "    print (f'\\n Classifier: {classifier}: \\n')\n",
    "    val_res = eval_val_from_npy(\n",
    "         val_npy_path = f\"/mnt/ephemeral0/{ATTACK}/val_raw.npy\",\n",
    "         val_label_path = f\"/mnt/ephemeral0/{ATTACK}/val_labels.npy\",\n",
    "         model_out_dir = f\"./{ATTACK}/model-{classifier}\",\n",
    "         return_predictions=True,\n",
    "         use_mmap=False,\n",
    "         csv_path=f\"./eval-VALSET/{ATTACK}-{ATTACK}-{IMG_SIZE}-{classifier}_test_result.csv\",\n",
    "         scaler_path=f\"./{ATTACK}/scaler.joblib\"\n",
    "    )\n",
    "    print(\"val metrics:\", val_res[\"metrics\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ea386d-f0e5-4527-8091-c8e21ed5285a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Method: bim\n",
      "\n",
      "\n",
      " Classifier: LogisticRegression: \n",
      "\n",
      "VAL Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "  confusion_matrix: [[81, 0], [0, 81]]\n",
      "Predictions saved to: eval-VALSET/bim-bim-1024-LogisticRegression_test_result_predictions.csv\n",
      "Metrics saved to: eval-VALSET/bim-bim-1024-LogisticRegression_test_result.csv\n",
      "val metrics: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0, 'confusion_matrix': [[81, 0], [0, 81]], 'classification_report': {'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 81.0}, '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 81.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 162.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 162.0}}}\n",
      "\n",
      " Classifier: RandomForest: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "  confusion_matrix: [[81, 0], [0, 81]]\n",
      "Predictions saved to: eval-VALSET/bim-bim-1024-RandomForest_test_result_predictions.csv\n",
      "Metrics saved to: eval-VALSET/bim-bim-1024-RandomForest_test_result.csv\n",
      "val metrics: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0, 'confusion_matrix': [[81, 0], [0, 81]], 'classification_report': {'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 81.0}, '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 81.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 162.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 162.0}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "ATTACK = 'bim'\n",
    "classifiers = ['LogisticRegression', 'RandomForest']\n",
    "IMG_SIZE = 1024\n",
    "\n",
    "print (f'Attack Method: {ATTACK}\\n')\n",
    "for classifier in classifiers:\n",
    "    print (f'\\n Classifier: {classifier}: \\n')\n",
    "    val_res = eval_val_from_npy(\n",
    "         val_npy_path = f\"./{ATTACK}/val_raw.npy\",\n",
    "         val_label_path = f\"./{ATTACK}/val_labels.npy\",\n",
    "         model_out_dir = f\"./{ATTACK}/model-{classifier}\",\n",
    "         return_predictions=True,\n",
    "         use_mmap=False,\n",
    "         csv_path=f\"./eval-VALSET/{ATTACK}-{ATTACK}-{IMG_SIZE}-{classifier}_test_result.csv\",\n",
    "         scaler_path=f\"./{ATTACK}/scaler.joblib\"\n",
    "    )\n",
    "    print(\"val metrics:\", val_res[\"metrics\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed43dfe4-d408-431e-b147-756ff66d9fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Method: pgd\n",
      "\n",
      "\n",
      " Classifier: LogisticRegression: \n",
      "\n",
      "VAL Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "  confusion_matrix: [[81, 0], [0, 81]]\n",
      "Predictions saved to: eval-VALSET/pgd-pgd-1024-LogisticRegression_test_result_predictions.csv\n",
      "Metrics saved to: eval-VALSET/pgd-pgd-1024-LogisticRegression_test_result.csv\n",
      "val metrics: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0, 'confusion_matrix': [[81, 0], [0, 81]], 'classification_report': {'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 81.0}, '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 81.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 162.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 162.0}}}\n",
      "\n",
      " Classifier: RandomForest: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "  confusion_matrix: [[81, 0], [0, 81]]\n",
      "Predictions saved to: eval-VALSET/pgd-pgd-1024-RandomForest_test_result_predictions.csv\n",
      "Metrics saved to: eval-VALSET/pgd-pgd-1024-RandomForest_test_result.csv\n",
      "val metrics: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0, 'confusion_matrix': [[81, 0], [0, 81]], 'classification_report': {'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 81.0}, '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 81.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 162.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 162.0}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "ATTACK = 'pgd'\n",
    "classifiers = ['LogisticRegression', 'RandomForest']\n",
    "IMG_SIZE = 1024\n",
    "\n",
    "print (f'Attack Method: {ATTACK}\\n')\n",
    "for classifier in classifiers:\n",
    "    print (f'\\n Classifier: {classifier}: \\n')\n",
    "    val_res = eval_val_from_npy(\n",
    "         val_npy_path = f\"./{ATTACK}/val_raw.npy\",\n",
    "         val_label_path = f\"./{ATTACK}/val_labels.npy\",\n",
    "         model_out_dir = f\"./{ATTACK}/model-{classifier}\",\n",
    "         return_predictions=True,\n",
    "         use_mmap=False,\n",
    "         csv_path=f\"./eval-VALSET/{ATTACK}-{ATTACK}-{IMG_SIZE}-{classifier}_test_result.csv\",\n",
    "         scaler_path=f\"./{ATTACK}/scaler.joblib\"\n",
    "    )\n",
    "    print(\"val metrics:\", val_res[\"metrics\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "478ae33c-f273-4187-b243-10c99881131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Method: df\n",
      "\n",
      "\n",
      " Classifier: LogisticRegression: \n",
      "\n",
      "VAL Eval results:\n",
      "  acc=0.5864  prec=0.5795  recall=0.6296  f1=0.6036\n",
      "  roc_auc=0.6030\n",
      "  confusion_matrix: [[44, 37], [30, 51]]\n",
      "Predictions saved to: eval-VALSET/df-df-1024-LogisticRegression_test_result_predictions.csv\n",
      "Metrics saved to: eval-VALSET/df-df-1024-LogisticRegression_test_result.csv\n",
      "val metrics: {'accuracy': 0.5864197530864198, 'precision': 0.5795454545454546, 'recall': 0.6296296296296297, 'f1': 0.6035502958579881, 'roc_auc': 0.6030330742264899, 'confusion_matrix': [[44, 37], [30, 51]], 'classification_report': {'0': {'precision': 0.5945945945945946, 'recall': 0.5432098765432098, 'f1-score': 0.567741935483871, 'support': 81.0}, '1': {'precision': 0.5795454545454546, 'recall': 0.6296296296296297, 'f1-score': 0.6035502958579881, 'support': 81.0}, 'accuracy': 0.5864197530864198, 'macro avg': {'precision': 0.5870700245700247, 'recall': 0.5864197530864197, 'f1-score': 0.5856461156709296, 'support': 162.0}, 'weighted avg': {'precision': 0.5870700245700246, 'recall': 0.5864197530864198, 'f1-score': 0.5856461156709295, 'support': 162.0}}}\n",
      "\n",
      " Classifier: RandomForest: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL Eval results:\n",
      "  acc=0.5062  prec=0.5040  recall=0.7778  f1=0.6117\n",
      "  roc_auc=0.4784\n",
      "  confusion_matrix: [[19, 62], [18, 63]]\n",
      "Predictions saved to: eval-VALSET/df-df-1024-RandomForest_test_result_predictions.csv\n",
      "Metrics saved to: eval-VALSET/df-df-1024-RandomForest_test_result.csv\n",
      "val metrics: {'accuracy': 0.5061728395061729, 'precision': 0.504, 'recall': 0.7777777777777778, 'f1': 0.6116504854368932, 'roc_auc': 0.478433165675964, 'confusion_matrix': [[19, 62], [18, 63]], 'classification_report': {'0': {'precision': 0.5135135135135135, 'recall': 0.2345679012345679, 'f1-score': 0.3220338983050847, 'support': 81.0}, '1': {'precision': 0.504, 'recall': 0.7777777777777778, 'f1-score': 0.6116504854368932, 'support': 81.0}, 'accuracy': 0.5061728395061729, 'macro avg': {'precision': 0.5087567567567568, 'recall': 0.5061728395061729, 'f1-score': 0.46684219187098897, 'support': 162.0}, 'weighted avg': {'precision': 0.5087567567567568, 'recall': 0.5061728395061729, 'f1-score': 0.46684219187098897, 'support': 162.0}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "ATTACK = 'df'\n",
    "classifiers = ['LogisticRegression', 'RandomForest']\n",
    "IMG_SIZE = 1024\n",
    "\n",
    "print (f'Attack Method: {ATTACK}\\n')\n",
    "for classifier in classifiers:\n",
    "    print (f'\\n Classifier: {classifier}: \\n')\n",
    "    val_res = eval_val_from_npy(\n",
    "         val_npy_path = f\"./{ATTACK}/val_raw.npy\",\n",
    "         val_label_path = f\"./{ATTACK}/val_labels.npy\",\n",
    "         model_out_dir = f\"./{ATTACK}/model-{classifier}\",\n",
    "         return_predictions=True,\n",
    "         use_mmap=False,\n",
    "         csv_path=f\"./eval-VALSET/{ATTACK}-{ATTACK}-{IMG_SIZE}-{classifier}_test_result.csv\",\n",
    "         scaler_path=f\"./{ATTACK}/scaler.joblib\"\n",
    "    )\n",
    "    print(\"val metrics:\", val_res[\"metrics\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd8a1a6-39a0-4611-816f-a419b119fbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Method: cw\n",
      "\n",
      "\n",
      " Classifier: LogisticRegression: \n",
      "\n",
      "VAL Eval results:\n",
      "  acc=0.5741  prec=0.5698  recall=0.6049  f1=0.5868\n",
      "  roc_auc=0.6024\n",
      "  confusion_matrix: [[44, 37], [32, 49]]\n",
      "Predictions saved to: eval-VALSET/cw-cw-1024-LogisticRegression_test_result_predictions.csv\n",
      "Metrics saved to: eval-VALSET/cw-cw-1024-LogisticRegression_test_result.csv\n",
      "val metrics: {'accuracy': 0.5740740740740741, 'precision': 0.5697674418604651, 'recall': 0.6049382716049383, 'f1': 0.5868263473053892, 'roc_auc': 0.6024234110653863, 'confusion_matrix': [[44, 37], [32, 49]], 'classification_report': {'0': {'precision': 0.5789473684210527, 'recall': 0.5432098765432098, 'f1-score': 0.5605095541401274, 'support': 81.0}, '1': {'precision': 0.5697674418604651, 'recall': 0.6049382716049383, 'f1-score': 0.5868263473053892, 'support': 81.0}, 'accuracy': 0.5740740740740741, 'macro avg': {'precision': 0.574357405140759, 'recall': 0.5740740740740741, 'f1-score': 0.5736679507227582, 'support': 162.0}, 'weighted avg': {'precision': 0.574357405140759, 'recall': 0.5740740740740741, 'f1-score': 0.5736679507227583, 'support': 162.0}}}\n",
      "\n",
      " Classifier: RandomForest: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL Eval results:\n",
      "  acc=0.5000  prec=0.5000  recall=0.2346  f1=0.3193\n",
      "  roc_auc=0.4709\n",
      "  confusion_matrix: [[62, 19], [62, 19]]\n",
      "Predictions saved to: eval-VALSET/cw-cw-1024-RandomForest_test_result_predictions.csv\n",
      "Metrics saved to: eval-VALSET/cw-cw-1024-RandomForest_test_result.csv\n",
      "val metrics: {'accuracy': 0.5, 'precision': 0.5, 'recall': 0.2345679012345679, 'f1': 0.31932773109243695, 'roc_auc': 0.4708885840573083, 'confusion_matrix': [[62, 19], [62, 19]], 'classification_report': {'0': {'precision': 0.5, 'recall': 0.7654320987654321, 'f1-score': 0.6048780487804878, 'support': 81.0}, '1': {'precision': 0.5, 'recall': 0.2345679012345679, 'f1-score': 0.31932773109243695, 'support': 81.0}, 'accuracy': 0.5, 'macro avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.46210288993646237, 'support': 162.0}, 'weighted avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.4621028899364623, 'support': 162.0}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "ATTACK = 'cw'\n",
    "classifiers = ['LogisticRegression', 'RandomForest']\n",
    "IMG_SIZE = 1024\n",
    "\n",
    "print (f'Attack Method: {ATTACK}\\n')\n",
    "for classifier in classifiers:\n",
    "    print (f'\\n Classifier: {classifier}: \\n')\n",
    "    val_res = eval_val_from_npy(\n",
    "         val_npy_path = f\"./{ATTACK}/val_raw.npy\",\n",
    "         val_label_path = f\"./{ATTACK}/val_labels.npy\",\n",
    "         model_out_dir = f\"./{ATTACK}/model-{classifier}\",\n",
    "         return_predictions=True,\n",
    "         use_mmap=False,\n",
    "         csv_path=f\"./eval-VALSET/{ATTACK}-{ATTACK}-{IMG_SIZE}-{classifier}_test_result.csv\",\n",
    "         scaler_path=f\"./{ATTACK}/scaler.joblib\"\n",
    "    )\n",
    "    print(\"val metrics:\", val_res[\"metrics\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ee11d-9f9d-4ece-a1f9-d066fae2354d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85173d7-a6e5-4a6f-bb52-1458aac32fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce989d-ef45-4d42-8c5f-0e9ffcf461d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d702779-6657-49b5-b6c1-448ae5ac88aa",
   "metadata": {},
   "source": [
    "# Evaluate Against Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ffaeacf-0557-4516-be76-275d3113936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: fgsm \n",
      " Dataset: fgsm \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.9817  prec=0.9877  recall=0.9756  f1=0.9816\n",
      "  roc_auc=0.9993\n",
      "Metrics saved to: Test-Eval/fgsm-fgsm-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: fgsm \n",
      " Dataset: bim \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.8293  prec=0.9821  recall=0.6707  f1=0.7971\n",
      "  roc_auc=0.9918\n",
      "Metrics saved to: Test-Eval/fgsm-bim-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: fgsm \n",
      " Dataset: pgd \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.6037  prec=0.9474  recall=0.2195  f1=0.3564\n",
      "  roc_auc=0.9795\n",
      "Metrics saved to: Test-Eval/fgsm-pgd-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: fgsm \n",
      " Dataset: df \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.5000  prec=0.5000  recall=0.0122  f1=0.0238\n",
      "  roc_auc=0.5009\n",
      "Metrics saved to: Test-Eval/fgsm-df-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: fgsm \n",
      " Dataset: cw \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.4939  prec=0.0000  recall=0.0000  f1=0.0000\n",
      "  roc_auc=0.5205\n",
      "Metrics saved to: Test-Eval/fgsm-cw-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: bim \n",
      " Dataset: fgsm \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.5244  prec=1.0000  recall=0.0488  f1=0.0930\n",
      "  roc_auc=0.9949\n",
      "Metrics saved to: Test-Eval/bim-fgsm-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: bim \n",
      " Dataset: bim \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "Metrics saved to: Test-Eval/bim-bim-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: bim \n",
      " Dataset: pgd \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "Metrics saved to: Test-Eval/bim-pgd-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: bim \n",
      " Dataset: df \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.5000  prec=0.0000  recall=0.0000  f1=0.0000\n",
      "  roc_auc=0.4960\n",
      "Metrics saved to: Test-Eval/bim-df-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: bim \n",
      " Dataset: cw \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.5000  prec=0.0000  recall=0.0000  f1=0.0000\n",
      "  roc_auc=0.5311\n",
      "Metrics saved to: Test-Eval/bim-cw-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: pgd \n",
      " Dataset: fgsm \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.5183  prec=1.0000  recall=0.0366  f1=0.0706\n",
      "  roc_auc=0.9787\n",
      "Metrics saved to: Test-Eval/pgd-fgsm-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: pgd \n",
      " Dataset: bim \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "Metrics saved to: Test-Eval/pgd-bim-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: pgd \n",
      " Dataset: pgd \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=1.0000  prec=1.0000  recall=1.0000  f1=1.0000\n",
      "  roc_auc=1.0000\n",
      "Metrics saved to: Test-Eval/pgd-pgd-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: pgd \n",
      " Dataset: df \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.5000  prec=0.0000  recall=0.0000  f1=0.0000\n",
      "  roc_auc=0.4914\n",
      "Metrics saved to: Test-Eval/pgd-df-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: pgd \n",
      " Dataset: cw \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.5000  prec=0.0000  recall=0.0000  f1=0.0000\n",
      "  roc_auc=0.5339\n",
      "Metrics saved to: Test-Eval/pgd-cw-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: df \n",
      " Dataset: fgsm \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.3598  prec=0.3231  recall=0.2561  f1=0.2857\n",
      "  roc_auc=0.2971\n",
      "Metrics saved to: Test-Eval/df-fgsm-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: df \n",
      " Dataset: bim \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.2317  prec=0.0000  recall=0.0000  f1=0.0000\n",
      "  roc_auc=0.0018\n",
      "Metrics saved to: Test-Eval/df-bim-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: df \n",
      " Dataset: pgd \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.2317  prec=0.0000  recall=0.0000  f1=0.0000\n",
      "  roc_auc=0.0012\n",
      "Metrics saved to: Test-Eval/df-pgd-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: df \n",
      " Dataset: df \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.5549  prec=0.5464  recall=0.6463  f1=0.5922\n",
      "  roc_auc=0.5848\n",
      "Metrics saved to: Test-Eval/df-df-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: df \n",
      " Dataset: cw \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.5427  prec=0.5368  recall=0.6220  f1=0.5763\n",
      "  roc_auc=0.5565\n",
      "Metrics saved to: Test-Eval/df-cw-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: cw \n",
      " Dataset: fgsm \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.7378  prec=0.6789  recall=0.9024  f1=0.7749\n",
      "  roc_auc=0.7885\n",
      "Metrics saved to: Test-Eval/cw-fgsm-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: cw \n",
      " Dataset: bim \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.6585  prec=0.6354  recall=0.7439  f1=0.6854\n",
      "  roc_auc=0.6762\n",
      "Metrics saved to: Test-Eval/cw-bim-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: cw \n",
      " Dataset: pgd \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.7012  prec=0.6602  recall=0.8293  f1=0.7351\n",
      "  roc_auc=0.7313\n",
      "Metrics saved to: Test-Eval/cw-pgd-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: cw \n",
      " Dataset: df \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.5305  prec=0.5333  recall=0.4878  f1=0.5096\n",
      "  roc_auc=0.5345\n",
      "Metrics saved to: Test-Eval/cw-df-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n",
      "Evaluate Against Test Set (Image Size: 1024) \n",
      " Method: cw \n",
      " Dataset: cw \n",
      " Classifier: LogisticRegression\n",
      "\n",
      "TEST Eval results:\n",
      "  acc=0.5366  prec=0.5395  recall=0.5000  f1=0.5190\n",
      "  roc_auc=0.5855\n",
      "Metrics saved to: Test-Eval/cw-cw-1024-LogisticRegression_test_result.csv\n",
      "_________________________\n",
      "\n",
      "===============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = ['LogisticRegression']\n",
    "IMG_SIZE = 1024\n",
    "\n",
    "ATTACKS = ['fgsm', 'bim', 'pgd', 'df', 'cw']\n",
    "DATASETS = ['fgsm', 'bim', 'pgd', 'df', 'cw']\n",
    "\n",
    "\n",
    "for ATTACK in ATTACKS:\n",
    "    for DATASET in DATASETS:\n",
    "        for classifier in classifiers:\n",
    "            print(f'Evaluate Against Test Set (Image Size: {IMG_SIZE}) \\n Method: {ATTACK} \\n Dataset: {DATASET} \\n Classifier: {classifier}\\n')\n",
    "            eval_test_from_npy(\n",
    "                test_clean_npy_path=f'/mnt/ephemeral0/{DATASET}/test_clean_raw.npy',\n",
    "                test_adv_npy_path=f'/mnt/ephemeral0/{DATASET}/test_adv_raw.npy',\n",
    "                model_out_dir=f\"/mnt/ephemeral0/{ATTACK}/model-{classifier}\",\n",
    "                test_labels_npy_path=None,\n",
    "                return_predictions=False,\n",
    "                use_mmap=False,\n",
    "                csv_path=f\"./Test-Eval/{ATTACK}-{DATASET}-{IMG_SIZE}-{classifier}_test_result.csv\",\n",
    "                scaler_path=f\"./{ATTACK}/scaler.joblib\"\n",
    "            )\n",
    "            print ('_________________________\\n')\n",
    "    \n",
    "        print ('===============================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c65f9-4f66-4d58-97ba-f5b868cf1bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
