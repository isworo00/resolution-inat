{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-11T20:39:34.386759Z",
     "iopub.status.busy": "2025-10-11T20:39:34.386203Z",
     "iopub.status.idle": "2025-10-11T20:39:42.894066Z",
     "shell.execute_reply": "2025-10-11T20:39:42.893465Z",
     "shell.execute_reply.started": "2025-10-11T20:39:34.386733Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import foolbox\n",
    "from foolbox import PyTorchModel\n",
    "from foolbox.attacks import (\n",
    "    LinfBasicIterativeAttack,\n",
    "    FGSM,\n",
    "    PGD,\n",
    "    L2DeepFoolAttack,\n",
    "    L2CarliniWagnerAttack,\n",
    ")\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def _load_checkpoint_into_model(model, checkpoint_path, map_location=\"cpu\", strict=True, verbose=False):\n",
    "    ckpt = torch.load(checkpoint_path, map_location=map_location)\n",
    "\n",
    "    if isinstance(ckpt, dict):\n",
    "        for candidate in (\"model_state\", \"model_state_dict\", \"state_dict\", \"state\"):\n",
    "            if candidate in ckpt:\n",
    "                state = ckpt[candidate]\n",
    "                break\n",
    "        else:\n",
    "            if all(isinstance(v, (torch.Tensor, type(None))) or hasattr(v, \"shape\") for v in ckpt.values()):\n",
    "                state = ckpt\n",
    "            else:\n",
    "                nested = None\n",
    "                for v in ckpt.values():\n",
    "                    if isinstance(v, dict):\n",
    "                        if nested is None or len(v) > len(nested):\n",
    "                            nested = v\n",
    "                state = nested if nested is not None else ckpt\n",
    "    else:\n",
    "        state = ckpt\n",
    "\n",
    "    if not isinstance(state, dict):\n",
    "        raise ValueError(f\"Checkpoint {checkpoint_path} does not contain a state-dict (found type: {type(state)})\")\n",
    "\n",
    "    keys = list(state.keys())\n",
    "    prefix = None\n",
    "    for p in (\"module.\", \"model.\"):\n",
    "        cnt = sum(1 for k in keys if k.startswith(p))\n",
    "        if cnt >= max(1, len(keys) // 2):\n",
    "            prefix = p\n",
    "            break\n",
    "\n",
    "    if prefix:\n",
    "        new_state = OrderedDict((k[len(prefix):], v) for k, v in state.items())\n",
    "    else:\n",
    "        new_state = state\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(new_state, strict=strict)\n",
    "        if verbose:\n",
    "            print(f\"Loaded checkpoint {checkpoint_path} (strict={strict}).\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Strict load failed: {e}. Trying non-strict load ...\")\n",
    "        res = model.load_state_dict(new_state, strict=False)\n",
    "        if verbose:\n",
    "            print(\"Loaded with strict=False. Missing keys:\", getattr(res, \"missing_keys\", None))\n",
    "            print(\"Unexpected keys:\", getattr(res, \"unexpected_keys\", None))\n",
    "        return model\n",
    "\n",
    "\n",
    "def run_attack(\n",
    "    model,\n",
    "    checkpoint_path,\n",
    "    preprocessing = dict(\n",
    "        mean=[0.4812775254249573, 0.4674863815307617, 0.4093940854072571],\n",
    "        std=[0.19709135591983795, 0.1933959424495697, 0.19051066040992737],\n",
    "        axis=-3\n",
    "    ),\n",
    "    attack=\"fgsm\",                  \n",
    "    csv_path=\"./data/clean_data.csv\",\n",
    "    imdir=\"./images\",\n",
    "    outdir=\"./data\",\n",
    "    use_cuda=True,\n",
    "    model_device=None,\n",
    "    load_map_location=None,\n",
    "    batch_size=32,\n",
    "    verbose=True,\n",
    "):\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    if model_device is None:\n",
    "        device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and use_cuda) else \"cpu\")\n",
    "    else:\n",
    "        device = torch.device(model_device)\n",
    "\n",
    "    if load_map_location is None:\n",
    "        ckpt_map = \"cpu\" if device.type == \"cpu\" else device\n",
    "    else:\n",
    "        ckpt_map = load_map_location\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Loading checkpoint:\", checkpoint_path, \"map_location:\", ckpt_map)\n",
    "    model = _load_checkpoint_into_model(model, checkpoint_path, map_location=ckpt_map, strict=True, verbose=verbose)\n",
    "\n",
    "    model = model.to(device).eval()\n",
    "    fmodel = PyTorchModel(model, bounds=(0, 1), preprocessing=preprocessing)\n",
    "\n",
    "    to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def load_image_tensor(rel_path, base_dir):\n",
    "        p = Path(base_dir) / str(rel_path).lstrip(\"/\")\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        return to_tensor(img)\n",
    "\n",
    "    if attack == \"fgsm\":\n",
    "        atk = FGSM()\n",
    "        epsilons = [0.03]\n",
    "    elif attack == \"bim\":\n",
    "        atk = LinfBasicIterativeAttack()\n",
    "        epsilons = [0.03]\n",
    "    elif attack == \"pgd\":\n",
    "        atk = PGD()\n",
    "        epsilons = [0.03]\n",
    "    elif attack == \"df\":\n",
    "        atk = L2DeepFoolAttack()\n",
    "        epsilons = None\n",
    "    elif attack == \"cw\":\n",
    "        atk = L2CarliniWagnerAttack(steps=1000)\n",
    "        epsilons = None\n",
    "    else:\n",
    "        raise ValueError(\"Unknown attack: \" + str(attack))\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df_correct = df[df[\"true_idx\"] == df[\"pred_idx\"]].copy()\n",
    "    df_correct['original_index'] = df_correct.index\n",
    "    total_candidates = len(df_correct)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"CSV: {csv_path} rows={len(df)}; correctly-classified candidates={total_candidates}; epsilons={epsilons}\")\n",
    "\n",
    "    out_base = Path(outdir)\n",
    "    out_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    attempted_count = 0\n",
    "    success_count = 0\n",
    "    saved_count = 0\n",
    "    \n",
    "    candidate_results = {}\n",
    "\n",
    "    for i in tqdm(range(0, total_candidates, batch_size), desc=\"Processing Batches\"):\n",
    "        batch_df = df_correct.iloc[i:i+batch_size]\n",
    "        \n",
    "        images, labels, batch_info = [], [], []\n",
    "        for _, row in batch_df.iterrows():\n",
    "            try:\n",
    "                img_tensor = load_image_tensor(row[\"rel_path\"], imdir)\n",
    "                images.append(img_tensor)\n",
    "                labels.append(int(row[\"true_idx\"]))\n",
    "                batch_info.append(row)\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Skipping image {row['rel_path']} due to loading error: {e}\")\n",
    "                candidate_results[row['original_index']] = {\"success\": False}\n",
    "        \n",
    "        if not images:\n",
    "            continue\n",
    "            \n",
    "        images_t = torch.stack(images).to(device)\n",
    "        labels_t = torch.tensor(labels, device=device)\n",
    "        \n",
    "        try:\n",
    "            _, advs_t, success_t = atk(fmodel, images_t, criterion=foolbox.criteria.Misclassification(labels_t), epsilons=epsilons)\n",
    "            attempted_count += len(images)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Attack failed for a batch: {e}\")\n",
    "            for row in batch_info:\n",
    "                candidate_results[row['original_index']] = {\"success\": False}\n",
    "            continue\n",
    "\n",
    "        for j, row in enumerate(batch_info):\n",
    "            original_idx = row['original_index']\n",
    "            \n",
    "            if epsilons is not None:\n",
    "                is_successful = bool(success_t[0][j].item())\n",
    "                adv_tensor_for_item = advs_t[0][j]\n",
    "            else: # For attacks like DeepFool where epsilons is None\n",
    "                is_successful = bool(success_t[j].item())\n",
    "                adv_tensor_for_item = advs_t[j]\n",
    "\n",
    "            candidate_results[original_idx] = {\"success\": is_successful}\n",
    "            \n",
    "            if is_successful:\n",
    "                success_count += 1\n",
    "                adv_cpu = adv_tensor_for_item.cpu()\n",
    "                \n",
    "                try:\n",
    "                    rel_clean = str(row[\"rel_path\"]).lstrip(\"/\")\n",
    "                    out_rel = Path(rel_clean).with_suffix(\".pt\")\n",
    "                    out_path = out_base / attack / out_rel\n",
    "                    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    torch.save(adv_cpu, out_path)\n",
    "                    saved_count += 1\n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"Failed to save adv tensor for {row['rel_path']}: {e}\")\n",
    "\n",
    "    df['success'] = df.index.map(lambda idx: candidate_results.get(idx, {}).get('success', False))\n",
    "    \n",
    "    meta_df = df[[\"rel_path\", \"true_idx\", \"pred_idx\", \"true_class\", \"pred_class\", \"success\"]]\n",
    "    meta_csv_path = out_base / f\"metadata_{attack}.csv\"\n",
    "    meta_df.to_csv(meta_csv_path, index=False)\n",
    "    if verbose:\n",
    "        print(f\"Wrote metadata CSV to: {meta_csv_path}\")\n",
    "\n",
    "    success_rate_over_candidates = success_count / total_candidates if total_candidates > 0 else 0.0\n",
    "    success_rate_over_attempts = success_count / attempted_count if attempted_count > 0 else 0.0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Attempted attacks: {attempted_count}/{total_candidates} candidates.\")\n",
    "        print(f\"Successes: {success_count}.\")\n",
    "        print(f\"Saved adv files: {saved_count}.\")\n",
    "        print(f\"Success rate (over candidates): {success_rate_over_candidates:.4f} ({success_count}/{total_candidates})\")\n",
    "        print(f\"Success rate (over attempted):  {success_rate_over_attempts:.4f} ({success_count}/{attempted_count if attempted_count > 0 else 0})\")\n",
    "\n",
    "    return success_rate_over_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T20:39:42.895614Z",
     "iopub.status.busy": "2025-10-11T20:39:42.895188Z",
     "iopub.status.idle": "2025-10-11T20:39:43.134833Z",
     "shell.execute_reply": "2025-10-11T20:39:43.134261Z",
     "shell.execute_reply.started": "2025-10-11T20:39:42.895587Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import py7zr\n",
    "\n",
    "def compress_generated_images(foldername):\n",
    "    src_dir = f'/kaggle/working/generated_images-test/{foldername}/'\n",
    "    output_path = f'/kaggle/working/generated_images-test/{foldername}_stored.7z'\n",
    "\n",
    "    if not os.path.exists(src_dir):\n",
    "        raise FileNotFoundError(f\"Source directory not found: {src_dir}\")\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "\n",
    "    with py7zr.SevenZipFile(\n",
    "        output_path,\n",
    "        mode='w',\n",
    "        filters=[{'id': py7zr.FILTER_COPY}]\n",
    "    ) as archive:\n",
    "        archive.writeall(src_dir, arcname='.')\n",
    "\n",
    "    print(f\"Successfully archived : {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T03:34:05.849735Z",
     "iopub.status.busy": "2025-10-11T03:34:05.849450Z",
     "iopub.status.idle": "2025-10-11T03:34:05.914259Z",
     "shell.execute_reply": "2025-10-11T03:34:05.913554Z",
     "shell.execute_reply.started": "2025-10-11T03:34:05.849712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved split_csv/correct_in_all_models_1.csv\n",
      "Saved split_csv/correct_in_all_models_2.csv\n",
      "Saved split_csv/correct_in_all_models_3.csv\n",
      "Saved split_csv/correct_in_all_models_4.csv\n",
      "Saved split_csv/correct_in_all_models_5.csv\n",
      "Saved split_csv/correct_in_all_models_6.csv\n",
      "Saved split_csv/correct_in_all_models_7.csv\n",
      "Saved split_csv/correct_in_all_models_8.csv\n",
      "Saved split_csv/correct_in_all_models_9.csv\n",
      "Saved split_csv/correct_in_all_models_10.csv\n",
      "Saved split_csv/correct_in_all_models_11.csv\n",
      "Saved split_csv/correct_in_all_models_12.csv\n",
      "Saved split_csv/correct_in_all_models_13.csv\n",
      "Saved split_csv/correct_in_all_models_14.csv\n",
      "Saved split_csv/correct_in_all_models_15.csv\n",
      "Saved split_csv/correct_in_all_models_16.csv\n",
      "Saved split_csv/correct_in_all_models_17.csv\n",
      "Saved split_csv/correct_in_all_models_18.csv\n",
      "Saved split_csv/correct_in_all_models_19.csv\n",
      "Saved split_csv/correct_in_all_models_20.csv\n",
      "Saved split_csv/correct_in_all_models_21.csv\n",
      "Saved split_csv/correct_in_all_models_22.csv\n",
      "Saved split_csv/correct_in_all_models_23.csv\n",
      "Saved split_csv/correct_in_all_models_24.csv\n",
      "Saved split_csv/correct_in_all_models_25.csv\n",
      "Saved split_csv/correct_in_all_models_26.csv\n",
      "Saved split_csv/correct_in_all_models_27.csv\n",
      "Saved split_csv/correct_in_all_models_28.csv\n",
      "Saved split_csv/correct_in_all_models_29.csv\n",
      "Saved split_csv/correct_in_all_models_30.csv\n",
      "Saved split_csv/correct_in_all_models_31.csv\n",
      "Saved split_csv/correct_in_all_models_32.csv\n",
      "Saved split_csv/correct_in_all_models_33.csv\n",
      "Saved split_csv/correct_in_all_models_34.csv\n",
      "Saved split_csv/correct_in_all_models_35.csv\n",
      "Saved split_csv/correct_in_all_models_36.csv\n",
      "Saved split_csv/correct_in_all_models_37.csv\n",
      "Saved split_csv/correct_in_all_models_38.csv\n",
      "Saved split_csv/correct_in_all_models_39.csv\n",
      "Saved split_csv/correct_in_all_models_40.csv\n",
      "Saved split_csv/correct_in_all_models_41.csv\n",
      "Saved split_csv/correct_in_all_models_42.csv\n",
      "Saved split_csv/correct_in_all_models_43.csv\n",
      "Saved split_csv/correct_in_all_models_44.csv\n",
      "Saved split_csv/correct_in_all_models_45.csv\n",
      "Saved split_csv/correct_in_all_models_46.csv\n",
      "Saved split_csv/correct_in_all_models_47.csv\n",
      "Saved split_csv/correct_in_all_models_48.csv\n",
      "Saved split_csv/correct_in_all_models_49.csv\n",
      "Saved split_csv/correct_in_all_models_50.csv\n",
      "Saved split_csv/correct_in_all_models_51.csv\n",
      "Saved split_csv/correct_in_all_models_52.csv\n",
      "Saved split_csv/correct_in_all_models_53.csv\n",
      "Saved split_csv/correct_in_all_models_54.csv\n",
      "Saved split_csv/correct_in_all_models_55.csv\n",
      "Saved split_csv/correct_in_all_models_56.csv\n",
      "Saved split_csv/correct_in_all_models_57.csv\n",
      "Saved split_csv/correct_in_all_models_58.csv\n",
      "Saved split_csv/correct_in_all_models_59.csv\n",
      "Saved split_csv/correct_in_all_models_60.csv\n",
      "Saved split_csv/correct_in_all_models_61.csv\n",
      "Saved split_csv/correct_in_all_models_62.csv\n",
      "Saved split_csv/correct_in_all_models_63.csv\n",
      "Saved split_csv/correct_in_all_models_64.csv\n",
      "Saved split_csv/correct_in_all_models_65.csv\n",
      "Saved split_csv/correct_in_all_models_66.csv\n",
      "Saved split_csv/correct_in_all_models_67.csv\n",
      "Saved split_csv/correct_in_all_models_68.csv\n",
      "Saved split_csv/correct_in_all_models_69.csv\n",
      "Saved split_csv/correct_in_all_models_70.csv\n",
      "Saved split_csv/correct_in_all_models_71.csv\n",
      "Saved split_csv/correct_in_all_models_72.csv\n",
      "Saved split_csv/correct_in_all_models_73.csv\n",
      "Saved split_csv/correct_in_all_models_74.csv\n",
      "Saved split_csv/correct_in_all_models_75.csv\n",
      "Done! All chunks saved.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "input_csv = '/kaggle/input/inatset/01-CleanModel/Evaluate/correct_in_all_models.csv'  \n",
    "rows_per_file = 30          \n",
    "output_dir = \"split_csv\"     \n",
    "\n",
    "Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "for i in range(0, len(df), rows_per_file):\n",
    "    chunk = df.iloc[i:i + rows_per_file]\n",
    "    chunk_file = Path(output_dir) / f\"correct_in_all_models_{i//rows_per_file + 1}.csv\"\n",
    "    chunk.to_csv(chunk_file, index=False)\n",
    "    print(f\"Saved {chunk_file}\")\n",
    "\n",
    "print(\"Done! All chunks saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T20:39:51.406575Z",
     "iopub.status.busy": "2025-10-11T20:39:51.406249Z",
     "iopub.status.idle": "2025-10-11T22:54:53.336432Z",
     "shell.execute_reply": "2025-10-11T22:54:53.335754Z",
     "shell.execute_reply.started": "2025-10-11T20:39:51.406554Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n",
      "100%|██████████| 528M/528M [00:03<00:00, 181MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: /kaggle/input/inatset/01-CleanModel/Models/512x512/best_min_acc_vgg16_512x512_Model-2.pth map_location: cuda:0\n",
      "Loaded checkpoint /kaggle/input/inatset/01-CleanModel/Models/512x512/best_min_acc_vgg16_512x512_Model-2.pth (strict=True).\n",
      "CSV: ./split_csv/correct_in_all_models_69.csv rows=30; correctly-classified candidates=30; epsilons=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 15/15 [46:08<00:00, 184.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata CSV to: generated_images-test/512x512-69/metadata_cw.csv\n",
      "Attempted attacks: 30/30 candidates.\n",
      "Successes: 30.\n",
      "Saved adv files: 30.\n",
      "Success rate (over candidates): 1.0000 (30/30)\n",
      "Success rate (over attempted):  1.0000 (30/30)\n",
      "Successfully archived : /kaggle/working/generated_images-test/512x512-69_stored.7z\n",
      "Loading checkpoint: /kaggle/input/inatset/01-CleanModel/Models/512x512/best_min_acc_vgg16_512x512_Model-2.pth map_location: cuda:0\n",
      "Loaded checkpoint /kaggle/input/inatset/01-CleanModel/Models/512x512/best_min_acc_vgg16_512x512_Model-2.pth (strict=True).\n",
      "CSV: ./split_csv/correct_in_all_models_70.csv rows=30; correctly-classified candidates=30; epsilons=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 15/15 [47:10<00:00, 188.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata CSV to: generated_images-test/512x512-70/metadata_cw.csv\n",
      "Attempted attacks: 30/30 candidates.\n",
      "Successes: 30.\n",
      "Saved adv files: 30.\n",
      "Success rate (over candidates): 1.0000 (30/30)\n",
      "Success rate (over attempted):  1.0000 (30/30)\n",
      "Successfully archived : /kaggle/working/generated_images-test/512x512-70_stored.7z\n",
      "Loading checkpoint: /kaggle/input/inatset/01-CleanModel/Models/512x512/best_min_acc_vgg16_512x512_Model-2.pth map_location: cuda:0\n",
      "Loaded checkpoint /kaggle/input/inatset/01-CleanModel/Models/512x512/best_min_acc_vgg16_512x512_Model-2.pth (strict=True).\n",
      "CSV: ./split_csv/correct_in_all_models_71.csv rows=30; correctly-classified candidates=30; epsilons=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 15/15 [41:32<00:00, 166.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata CSV to: generated_images-test/512x512-71/metadata_cw.csv\n",
      "Attempted attacks: 30/30 candidates.\n",
      "Successes: 30.\n",
      "Saved adv files: 30.\n",
      "Success rate (over candidates): 1.0000 (30/30)\n",
      "Success rate (over attempted):  1.0000 (30/30)\n",
      "Successfully archived : /kaggle/working/generated_images-test/512x512-71_stored.7z\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import VGG16_BN_Weights\n",
    "from torchvision import models, transforms\n",
    "\n",
    "model = models.vgg16_bn(weights=VGG16_BN_Weights.IMAGENET1K_V1)\n",
    "model.avgpool = nn.AdaptiveAvgPool2d((7,7))\n",
    "model.classifier[6] = nn.Linear(4096, 7)\n",
    "\n",
    "img_size = 512\n",
    "checkpoint_path = f'/kaggle/input/inatset/01-CleanModel/Models/{img_size}x{img_size}/best_min_acc_vgg16_{img_size}x{img_size}_Model-2.pth'\n",
    "imdir = f'/kaggle/input/inatset/01-CleanModel/Dataset/{img_size}x{img_size}'\n",
    "outdir = f'./generated_images-test/{img_size}x{img_size}'\n",
    "\n",
    "attack_method = 'cw'\n",
    "for i in range (69,72):\n",
    "    detect_csv_split = f'./split_csv/correct_in_all_models_{i}.csv'\n",
    "    foldername = f'{outdir}-{i}'\n",
    "    run_attack(\n",
    "        model=model,\n",
    "        checkpoint_path=checkpoint_path,                  \n",
    "        attack=\"cw\",                    \n",
    "        csv_path=detect_csv_split,\n",
    "        imdir=imdir,\n",
    "        outdir=foldername,\n",
    "        batch_size=2,\n",
    "        verbose = True)\n",
    "    compress_generated_images (f'512x512-{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8424941,
     "sourceId": 13292693,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
